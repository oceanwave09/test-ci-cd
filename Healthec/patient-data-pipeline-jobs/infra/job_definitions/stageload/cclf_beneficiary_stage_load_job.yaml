apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: cclf-bene-${ENTITYNAME}-stage-load-job-$RANDOM_ID
  namespace: data-pipeline
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: "registry.gitlab.com/health-ec/platform/domain/member/etl/patient-data-pipeline-jobs/patient-data-pipeline:latest"
  imagePullPolicy: Always
  imagePullSecrets:
    - gitlab-patient-datapipeline-regcred
  mainApplicationFile: local:///app/jobs/stageload/cclf_beneficiary/cclf_bene_${ENTITY}_stage_load_job.py
  deps:
    pyFiles:
      - local:///app/python-deps.zip
  sparkVersion: "3.4.1"
  timeToLiveSeconds: 3600
  nodeSelector:
    role: pyspark_jobs
  restartPolicy:
    type: Never
  driver:
    cores: 1
    memory: "4096m"
    labels:
      version: 3.4.1
      auto_scaling_groups: pyspark_jobs
    serviceAccount: spark-operator-spark
    env:
      - name: DELTA_SCHEMA_LOCATION
        valueFrom:
          configMapKeyRef:
            name: ingestion-pipeline-config
            key: ${TENANT}.delta_schema_location
      - name: DELTA_TABLE_NAME
        valueFrom:
          configMapKeyRef:
            name: ingestion-pipeline-config
            key: ${TENANT}.${ENTITY}_table_name
            optional: true
      - name: CANONICAL_FILE_PATH
        value: $CANONICAL_FILE_PATH
      - name: FILE_BATCH_ID
        value: $FILE_BATCH_ID
      - name: FILE_SOURCE
        value: $FILE_SOURCE
      - name: FILE_NAME
        value: $FILE_NAME
    tolerations:
      - key: dedicated
        operator: "Equal"
        value: pyspark_jobs
        effect: NoSchedule
  executor:
    cores: 1
    instances: 1
    memory: "4096m"
    labels:
      version: 3.4.1
      auto_scaling_groups: pyspark_jobs
    env:
      - name: DELTA_SCHEMA_LOCATION
        valueFrom:
          configMapKeyRef:
            name: ingestion-pipeline-config
            key: ${TENANT}.delta_schema_location
      - name: DELTA_TABLE_NAME
        valueFrom:
          configMapKeyRef:
            name: ingestion-pipeline-config
            key: ${TENANT}.gap_${ENTITY}_table_name
            optional: true
      - name: CANONICAL_FILE_PATH
        value: $CANONICAL_FILE_PATH
      - name: FILE_BATCH_ID
        value: $FILE_BATCH_ID
      - name: FILE_SOURCE
        value: $FILE_SOURCE
      - name: FILE_NAME
        value: $FILE_NAME
    tolerations:
      - key: dedicated
        operator: "Equal"
        value: pyspark_jobs
        effect: NoSchedule
