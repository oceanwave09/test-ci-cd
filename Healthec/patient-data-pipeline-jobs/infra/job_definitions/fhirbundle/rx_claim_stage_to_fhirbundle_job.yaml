apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: rx-claim-stage-to-fhirbundle-job-$RANDOM_ID
  namespace: data-pipeline
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: "registry.gitlab.com/health-ec/platform/domain/member/etl/patient-data-pipeline-jobs/patient-data-pipeline:latest"
  imagePullPolicy: Always
  imagePullSecrets:
    - gitlab-patient-datapipeline-regcred
  mainApplicationFile: local:///app/jobs/fhirbundle/rx_claim_stage_to_fhirbundle_job.py
  deps:
    pyFiles:
      - local:///app/python-deps.zip
  sparkVersion: "3.4.1"
  timeToLiveSeconds: 3600
  nodeSelector:
    role: pyspark_jobs
  restartPolicy:
    type: Never
  driver:
    cores: 1
    memory: "4096m"
    labels:
      version: 3.4.1
      auto_scaling_groups: pyspark_jobs
    serviceAccount: spark-operator-spark
    env:
      - name: DELTA_SCHEMA_LOCATION
        valueFrom:
          configMapKeyRef:
            name: ingestion-pipeline-config
            key: ${TENANT}.delta_schema_location
      - name: DELTA_TABLE_NAME
        valueFrom:
          configMapKeyRef:
            name: ingestion-pipeline-config
            key: ${TENANT}.rx_claim_table_name
            optional: true
      - name: LANDING_PATH
        valueFrom:
          configMapKeyRef:
            name: ingestion-pipeline-config
            key: ${TENANT}.landing_path
      - name: PIPELINE_DATA_KEY
        valueFrom:
          secretKeyRef:
            name: ${TENANT}-data-key
            key: pipeline_data_key
      - name: FILE_TENANT
        value: $FILE_TENANT
      - name: FILE_SOURCE
        value: $FILE_SOURCE
      - name: RESOURCE_TYPE
        value: $RESOURCE_TYPE
      - name: FILE_BATCH_ID
        value: $FILE_BATCH_ID
      - name: SRC_FILE_NAME
        value: $SRC_FILE_NAME
      - name: SRC_ORGANIZATION_ID
        value: $SRC_ORGANIZATION_ID
    tolerations:
      - key: dedicated
        operator: "Equal"
        value: pyspark_jobs
        effect: NoSchedule
  executor:
    cores: 1
    instances: 1
    memory: "4096m"
    labels:
      version: 3.4.1
      auto_scaling_groups: pyspark_jobs
    env:
      - name: DELTA_SCHEMA_LOCATION
        valueFrom:
          configMapKeyRef:
            name: ingestion-pipeline-config
            key: ${TENANT}.delta_schema_location
      - name: DELTA_TABLE_NAME
        valueFrom:
          configMapKeyRef:
            name: ingestion-pipeline-config
            key: ${TENANT}.rx_claim_table_name
            optional: true
      - name: LANDING_PATH
        valueFrom:
          configMapKeyRef:
            name: ingestion-pipeline-config
            key: ${TENANT}.landing_path
      - name: PIPELINE_DATA_KEY
        valueFrom:
          secretKeyRef:
            name: ${TENANT}-data-key
            key: pipeline_data_key
      - name: FILE_TENANT
        value: $FILE_TENANT
      - name: FILE_SOURCE
        value: $FILE_SOURCE
      - name: RESOURCE_TYPE
        value: $RESOURCE_TYPE
      - name: FILE_BATCH_ID
        value: $FILE_BATCH_ID
      - name: SRC_FILE_NAME
        value: $SRC_FILE_NAME
      - name: SRC_ORGANIZATION_ID
        value: $SRC_ORGANIZATION_ID
    tolerations:
      - key: dedicated
        operator: "Equal"
        value: pyspark_jobs
        effect: NoSchedule
