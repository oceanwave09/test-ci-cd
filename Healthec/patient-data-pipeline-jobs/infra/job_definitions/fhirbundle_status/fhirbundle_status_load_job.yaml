apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: fhirbundle-status-load-job-$RANDOM_ID
  namespace: data-pipeline
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: "registry.gitlab.com/health-ec/platform/domain/member/etl/patient-data-pipeline-jobs/patient-data-pipeline:latest"
  imagePullPolicy: Always
  imagePullSecrets:
    - gitlab-patient-datapipeline-regcred
  mainApplicationFile: local:///app/jobs/fhirbundle_status/fhirbundle_status_load_job.py
  deps:
    pyFiles:
      - local:///app/python-deps.zip
  sparkVersion: "3.4.1"
  timeToLiveSeconds: 3600
  nodeSelector:
    role: pyspark_jobs
  restartPolicy:
    type: Never
  driver:
    cores: 1
    memory: "4096m"
    labels:
      version: 3.4.1
      auto_scaling_groups: pyspark_jobs
    serviceAccount: spark-operator-spark
    env:
      - name: DELTA_SCHEMA_LOCATION
        valueFrom:
          configMapKeyRef:
            name: ingestion-pipeline-config
            key: ${TENANT}.delta_schema_location
      - name: DELTA_TABLE_NAME
        valueFrom:
          configMapKeyRef:
            name: ingestion-pipeline-config
            key: ${TENANT}.fhirbundle_status_table_name
            optional: true
      - name: EVENT_FILE_PATH
        value: $EVENT_FILE_PATH
    tolerations:
      - key: dedicated
        operator: "Equal"
        value: pyspark_jobs
        effect: NoSchedule
  executor:
    cores: 1
    instances: 1
    memory: "4096m"
    labels:
      version: 3.4.1
      auto_scaling_groups: pyspark_jobs
    env:
      - name: DELTA_SCHEMA_LOCATION
        valueFrom:
          configMapKeyRef:
            name: ingestion-pipeline-config
            key: ${TENANT}.delta_schema_location
      - name: DELTA_TABLE_NAME
        valueFrom:
          configMapKeyRef:
            name: ingestion-pipeline-config
            key: ${TENANT}.fhirbundle_status_table_name
            optional: true
      - name: EVENT_FILE_PATH
        value: $EVENT_FILE_PATH
    tolerations:
      - key: dedicated
        operator: "Equal"
        value: pyspark_jobs
        effect: NoSchedule
